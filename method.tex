\chapter{Methode}
\label{chap:method}
Dieses Kapitel erläutert die Art und Weise, mit der aus den im Experiment gewonnenen Daten Modelle trainiert wurden, welche die ausgeführte Aktivität automatisch erkennen können. Der erste Schritt ist die Aufnahme der Rohdaten, die durch eine in Abschnitt~\ref{sec:android-app} beschriebene Android-App realisiert wurde. Die gespeicherten Rohdaten sind noch nicht für einen herkömmlichen ML-Klassifikationsalgorithmus verwertbar, weshalb eine in Abschnitt~\ref{sec:transformation} beschriebene Transformation ausgeführt wird. Als dritter und letzter Schritt erfolgt die Ausführung diverser ML-Klassifikationsalgorithmen, die in Abschnitt~\ref{section:ml-algos} erläutert werden.

\section{Implementierung der Aufzeichnungssoftware}
\label{sec:android-app}
Die Aufzeichnung der Daten erforderte die Entwicklung einer neuen Software. Mit dem \textit{Actitracker} besteht zwar bereits eine Software von Lockhart et al. \cite{Lockhart2014}, jedoch unterstützt diese die synchronisierte Aufnahme mehrerer Sensoren sowie das Microsoft Band 2 generell nicht. Wichtige Anforderungen an die Aufzeichnungssoftware waren in dieser Arbeit eine hohe Zuverlässigkeit, Fernsteuerbarkeit und das automatische Hochladen der Daten auf einen Datenspeicher im Anschluss an die Aufnahme. Teilnehmer des Experimentes sollten mit der Anwendung möglichst wenig in Berührung kommen.

\subsection{Definition der Messdaten}
\begin{definition}
Ein \textit{Reading} besteht aus einem Unix-Zeitstempel in Millisekunden, einer Gerätequelle, einem Sensortyp und den Werten des Sensors gruppiert nach Komponente. Ein Beispiel ist $(1000, \text{Band}, \text{Gyroscope}, \{x \to 0.0, y \to 1.0, z \to 1.0\})$.
\end{definition}
\subsection{Struktur der Aufzeichnungssoftware}
Die Aufzeichnungssoftware wurde als Anwendung für das Android-Betriebssystem implementiert, da für diese Plattform auch ein \textit{Software Development Kit (SDK)} für das Microsoft Band 2 existiert.
\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth, clip, trim=6mm 6mm 9mm 6mm]{img/app-structure}
\caption{Struktur der Aufzeichnungssoftware}
\label{fig:app-structure}
\end{figure}

Abbildung~\ref{fig:app-structure} bietet einen Überblick über die Struktur der Aufzeichnungssoftware. Im Zentrum steht die Klasse \textit{CollectionService}, die für die robuste Sammlung der Daten verantwortlich ist und daher als persistenter Android-\textit{Service} implementiert ist, der im Gegensatz zu anderen Komponenten einer Android-App nicht ohne weiteres durch das System zur Schonung der Ressourcen beendet werden kann. Gesteuert wird die Aufnahme über eine grafische Benutzeroberfläche auf dem Smartphone selbst (\textit{AndroidUI}) sowie optional auch über eine Weboberfläche, die über einen einfachen HTTP-Server zur Verfügung steht. In den Benutzeroberflächen können der Name des Probanden sowie die ausgeführte Aktivität angegeben werden. Des Weiteren wird hierüber die Aufnahme gestartet und gestoppt.

Der \textit{CollectionService} delegiert die Aufnahme an den \textit{BandDataRecorder} und den \textit{LocalDataRecorder} weiter, die jeweils für das Microsoft Band 2 respektive die Smartphone-lokalen Sensoren verantwortlich sind. Über den \textit{BandConnector} wird die Bluetooth-Verbindung zum Band hergestellt und aufrecht erhalten. \textit{BandDataRecorder} und \textit{LocalDataRecorder} zeichnen Daten in Form von \textit{Readings} auf. Die Methode \textit{getValuesPerComponent()} liefert bei einem triaxialen Sensor beispielsweise Daten der Form $\{x \to 1.0, y \to 2.0, z \to 3.0\}$.

Wird die Aufnahme gestoppt, werden die Readings mitsamt den Metadaten Name des Probanden und Aktivität serialisiert und per SFTP auf einen entfernten Speicher hochgeladen. Anschließend werden die Daten von dort abgerufen und wie im folgenden Abschnitt beschrieben weiterverarbeitet.

\section{Transformation der Daten}
\label{sec:transformation}
Jede einzelne Aufnahme eines Nutzers und einer Aktivität besteht aus einer Reihe von Daten mit Zeitstempeln. In dieser Form kann noch kein konventioneller ML\hyph Klassifikationsalgorithmus mit den Daten arbeiten, da diese Daten in Form einer Liste von \textit{Instanzen} erwarten. Eine Instanz besteht aus einer Menge von \textit{Attributen/Features}, die im Rahmen dieser Arbeit bis auf das nominale \textit{Target}-Attribut allesamt kardinal sind. Das Target-Attribut ist dabei die durchgeführte Aktivität. Entsprechend ist eine Transformation der Daten notwendig, um diese tatsächlich nutzbar zu machen.

Um Instanzen zu erzeugen, teilt die Transformationssoftware die Ursprungsdaten zunächst in Intervalle mit zehnsekündiger Dauer auf. Ein solches Intervall soll schließlich eine Instanz, d.h. ein Beispiel für eine Aktivität bilden. Das erste und das letzte Intervall werden gelöscht, da in dieser Zeit die Aufnahme gestartet und gestoppt wurde, was ansonsten durch den anderen Bewegungsablauf die Ergebnisse verfälschen könnte. Aktuell umfasst das Intervall noch mehrere Readings, die aggregiert werden müssen.

\subsection{Beschreibung der Aggregatfunktionen}
Um aus den sequentiellen Daten des Intervalls eine feste Anzahl von Features zu gewinnen, die dieses Intervall beschreiben, werden Aggregatfunktionen benötigt, die eine Liste von Zahlen $A[1..N]$ auf eine feste Anzahl von Werten reduziert, welche die sequentiellen Daten in Kombination möglichst gut beschreiben. In den folgenden Abschnitten werden die auch von Weiss et al. verwendeten \cite{Weiss2016} Aggregatfunktionen in Anlehnung an Kwapisz et al. \cite{Kwapisz2011} begründet und definiert.
\subsubsection{Average \& Standard Deviation}
Der Durchschnittswert einer Reihe von Zahlen ist im Kontext der Aktivitätenerkennung relevant, da er zusammen mit der ebenfalls nachfolgend definierten Standardabweichung auf einfache Art und Weise angibt, in welchen Bereichen sich die Sensordaten bewegen.
\begin{align*}
\text{Avg}(A) &:= \frac{1}{N} \cdot \sum_{a \in A} a \\
\text{StdDev}(A) &:= \sqrt{\frac{1}{N} \cdot \sum_{a \in A} (a - \text{Avg}(A))^2}
\end{align*}
\subsubsection{Average Absolute Difference}
Diese Funktion gibt ähnlich wie \textit{StdDev} Veränderungen in den Datenreihen wieder, ist dabei jedoch nicht quadratisch und somit für ML-Algorithmen möglicherweise leichter zu handhaben.
\[
\text{AvgAbsDiff}(A) := \frac{1}{N} \cdot \sum_{a \in A} |a - \text{Avg}(A)|
\]
\subsubsection{$k$-Binned Distribution}
Im Gegensatz zu den obigen Funktionen liefert diese Aggregatfunktion einen $k$-Vektor anstatt eines Skalars. Wie \textit{Avg} und \textit{StdDev} erklärt diese Funktion den Wertebereich der Datenreihen, deckt diesen allerdings vollständig ab. Da die Interpretation der Ergebnisse dieser Funktion dafür komplexer ist, handelt es sich um eine sinnvolle Ergänzung zu den genannten Funktionen.
\\\\
Seien $i \in \{0, ..., k - 1\}, S := \frac{\max A - \min A}{k}$. Dann ist die \textit{Binned Distribution} wie folgt definiert:
\[
\text{Bin}(A, i) := \#\{a \in A | (\min A) + i \cdot S \leq a < (\min A) + (i + 1) \cdot S\}
\]
$\text{Bin}(A, i)$ ist demzufolge die Anzahl der Elemente in Korb $i$, wenn man $A$ der Größe nach sortiert auf $k$ Körbe verteilt. Der Bin ist hierbei als Multimenge zu verstehen und darf somit Elemente auch mehrfach enthalten.
\subsubsection{Average Root of Squares}
Auch diese Aggregatfunktion unterscheidet sich von den bisherigen, da sie mehrere Listen als Parameter erhält und somit Zusammenhänge zwischen diesen erklärt:
\[
\text{Aros}(A_1, ..., A_M) = \frac{1}{M} \cdot \sum_{i = 1}^{M} \sqrt{\sum_{a \in A_i} a^2}
\]
\subsubsection{Average Time between Peaks}
Diese Aggregatfunktion gibt die durchschnittliche Zeit zwischen Höhepunkten in $A$ zurück, die ein signifikantes Merkmal körperlicher Aktivitäten sein können, wofür zusätzlich eine Funktion $\text{timeForIndex}: \mathbb{N} \to \mathbb{N}$ benötigt wird. Die Wahl der Höhepunkte erfolgt durch eine Heuristik. 

Algorithmus~\ref{algo:avgTimeBetweenPeaks} beschreibt die Berechnung des Wertes. Zunächst wird mittels Algorithmus~\ref{algo:indicesOfPeaks} bestimmt, an welchen Indizes in der Liste Werte stehen, die deutlich größer als ihre Nachfolger sind. Dadurch wird definiert, was eine Spitze in dieser spezifischen Liste ausmacht. Anschließend werden Spitzen gesucht, die maximal um den Faktor \textit{threshold} kleiner sind als die größte Spitze, die der Algorithmus gefunden hast. Diese Grenze wird solange gesenkt, bis genügend Spitzen gefunden wurden oder die Grenze so niedrig liegt, dass es sich nicht mehr um Spitzen handelt. Als letztes berechnet der Algorithmus die durchschnittliche Zeit zwischen den Spitzen mit Hilfe der Funktion \textit{timeForIndex}.

Die im Pseudocode verwendeten Konstanten erwiesen sich im praktischen Test an den gesammelten Daten als gut geeignet. Für andere Datensätze kann eine Anpassung erforderlich sein.

\begin{algorithm}
    \caption{IndicesOfPeaks($A$, $t$), $t \in [0,1]$. Returns a list of indices $i$ where $A[i] \cdot t \geq A[i+1]$}
    \label{algo:indicesOfPeaks}
    \begin{algorithmic}
        \State $\text{indices} \gets \emptyset$
        \For{$i \in \{1, ..., |A| - 1\}$}
            \If{$A[i] \cdot t \geq A[i+1]$}
                \State $\text{indices} \gets \text{indices} \cup \{i\}$
            \EndIf
        \EndFor
        \State \Return $\text{indices}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{AverageTimeBetweenPeaks($A, \text{timeForIndex}, \text{minPeaks}$)}
    \label{algo:avgTimeBetweenPeaks}
    \begin{algorithmic}
        \State \LeftComment \textit{First, heuristically define what a peak is}
        \State $\text{indicesOfPeaks} \gets \text{IndicesOfPeaks}(A, t = 0.8)$
        \If{$\text{indicesOfPeaks} = \emptyset$}
            \State \Return Nil
        \EndIf
        \State $\text{highestPeakIdx} \gets$ Index of highest peak in indicesOfPeaks
        \State \LeftComment{\textit{Lower the threshold until we have found enough peaks or the threshold is too low}}
        \State $\text{otherPeakIndices} \gets \emptyset$
        \State threshold $\gets 0.8$
        \Repeat
            \State otherPeakIndices $\gets$ Indices of $A$ where $A[i] \geq A[\text{highestPeakIdx}] \cdot $ threshold
            \State threshold $\gets \text{threshold} \cdot 0.9$
        \Until{$|\text{otherPeaksIndices}| \geq \text{minPeaks} \vee \text{threshold} < 0.3$}
        \State \LeftComment \textit{Check whether we have found enough peaks}
        \If{$|\text{otherPeakIndices}| < $ minPeaks}
            \State \Return Nil
        \EndIf
        \State \LeftComment \textit{Calculate the average time between the peaks}
        \State timesBetweenPeaks $\gets \emptyset$
        \For{$i \in \{2, ..., |\text{otherPeakIndices}|\}$}
            \State time $\gets$ timeForIndex(otherPeakIndices[$i$]) - timeForIndex(otherPeakIndices[$i - 1$])
            \State timesBetweenPeaks $\gets$ timesBetweenPeaks $\cup \{\text{time}\}$
        \EndFor
        \State \Return \Call{Avg}{timesBetweenPeaks}
    \end{algorithmic}
\end{algorithm}

\subsection{Anwendung der Aggregatfunktionen}
Die Transformationssoftware reduziert nun mit Hilfe der Aggregatfunktionen die Readings eines jeden Intervalls $I$ auf eine konstante Anzahl skalarer Werte, welche die Features der Instanz darstellen. Im Folgenden sei $I$ ein Array der Struktur $I[G][S][K] \in \mathbb{R}^\alpha$, das die Messungen gruppiert nach Gerät ($G$), Sensor ($S$) und Komponente ($K$) beinhaltet. Des Weiteren sei $F_I$ die Menge der Features des Intervalls $I$. Die Folgenden Unterabschnitte zeigen den Aufbau von $F_I$ mittels der Aggregatfunktionen:
\subsubsection{Pro Kombination von Gerät $G$ und Sensor $S$}
Seien $P$ die einzelnen Komponenten des Sensors, beispielsweise $P = \{x, y, z\}$.
\begin{align*}
    \text{aros} &\gets \text{Aros}(I[G][S][P[1]], ..., I[G][S][P[|P|]]) \\
    F_I &\gets F_I \cup \{((G, S), \text{aros})\}
\end{align*}
Im Kontext betrachtet liefert \textit{Aros} am Beispiel des Beschleunigungssensors gewissermaßen die durchschnittliche Beschleunigung, die alle Achsen zusammen erfahren haben.
\subsubsection{Pro Kombination von Gerät $G$, Sensor $S$ und Komponente $K$}
Sei \textit{timeForIdx} hier eine Funktion, die den Zeitstempel des jeweiligen Datums im Intervall zurückgibt. Sei außerdem $k := 10$, sodass 10 Körbe verwendet werden, was sich experimentell als geeignet herausstellte.
\begin{align*}
    F_I &\gets F_I \cup \{((G, S, K), \text{Avg}(I[G][S][K]))\} \\
    F_I &\gets F_I \cup \{((G, S, K), \text{StdDev}(I[G][S][K]))\} \\
    F_I &\gets F_I \cup \{((G, S, K), \text{AvgAbsDiff}(I[G][S][K]))\} \\
    F_I &\gets F_I \cup \{((G, S, K), \text{AverageTimeBetweenPeaks}(I[G][S][K], \text{timeForIdx}, 3))\} \\
    F_I &\gets F_I \cup \{((G, S, K), \text{Bin}(I[G][S][K], i))\} \forall i \in \{1, ..., k\}
\end{align*}

\subsection{Beispiel}
Wir betrachten für die Aktivität \textit{Jogging} den Beispieldatensatz
\begin{align*}
D = \{&(0, \text{Band}, \text{Gyroscope}, {x \to 1.0, y \to 0.0}), \\
&(0, \text{Band}, \text{Accelerometer}, {x \to 1.0, y \to 1.0}), \\
&(1000, \text{Band}, \text{Gyroscope}, {x \to 2.0, y \to 2.0}), \\
&(1000, \text{Band}, \text{Accelerometer}, {x \to 0.0, y \to 1.0}), \\
&(2000, \text{Band}, \text{Gyroscope}, {x \to 3.0, y \to 0.0}), \\
&(2000, \text{Band}, \text{Accelerometer}, {x \to 1.0, y \to 3.0})\}
\end{align*} 
Zur Vereinfachung wird von biaxialen Sensoren ausgegangen, sodass keine $z$-Komponente vorhanden ist. Als Aggregatfunktionen werden der Durchschnitt, die \textit{Binned Distribution} mit zwei \textit{Bins} und der \textit{Average Root of Squares} verwendet. Die gewählte Intervallgröße beträgt zwei Sekunden, sodass sich die folgenden Intervalle ergeben:
\begin{align*}
I_1 = \{&(0, \text{Band}, \text{Gyroscope}, {x \to 1.0, y \to 0.0}), \\
&(0, \text{Band}, \text{Accelerometer}, {x \to 1.0, y \to 1.0}), \\
&(1000, \text{Band}, \text{Gyroscope}, {x \to 2.0, y \to 2.0}), \\
&(1000, \text{Band}, \text{Accelerometer}, {x \to 0.0, y \to 1.0})\}, \\
I_2 = \{&(2000, \text{Band}, \text{Gyroscope}, {x \to 3.0, y \to 0.0}), \\
&(2000, \text{Band}, \text{Accelerometer}, {x \to 1.0, y \to 3.0})\}
\end{align*}
Nun werden die Aggregatfunktionen auf $I_1$ angewendet:
\begin{align*}
&\text{Avg}_\text{Band, Gyroscope, x} = (1 + 2)/2 = 1.5 \\
&\text{Avg}_\text{Band, Gyroscope, y} = (0 + 2)/2 = 1 \\
&\text{Avg}_\text{Band, Accelerometer, x} = (1 + 0)/2 = 0.5 \\
&\text{Avg}_\text{Band, Accelerometer, y} = (1 + 1)/2 = 1 \\
&\text{Bin}_\text{Band, Gyroscope, x}(0) = \#\{1\} = 1 \\
&\text{Bin}_\text{Band, Gyroscope, x}(1) = \#\{2\} = 1 \\
&\text{Bin}_\text{Band, Gyroscope, y}(0) = \#\{0\} = 1 \\
&\text{Bin}_\text{Band, Gyroscope, y}(1) = \#\{2\} = 1 \\
&\text{Bin}_\text{Band, Accelerometer, x}(0) = \#\{0\} = 1 \\
&\text{Bin}_\text{Band, Accelerometer, x}(1) = \#\{1\} = 1 \\
&\text{Bin}_\text{Band, Accelerometer, y}(0) = \#\{\} = 0 \\
&\text{Bin}_\text{Band, Accelerometer, y}(1) = \#\{\} = 0 \\
&\text{Aros}_\text{Band, Gyroscope} = (1/2) \cdot (\underbrace{\sqrt{1^2 + 2^2}}_x + \underbrace{\sqrt{0^2 + 2^2}}_y) \approx 2.12 \\
&\text{Aros}_\text{Band, Accelerometer} = (1/2) \cdot (\underbrace{\sqrt{1^2 + 0^2}}_x + \underbrace{\sqrt{1^2 + 1^2}}_y) \approx 1.2 \\
\end{align*}
Die Berechnung für $I_2$ erfolgt analog. Aus den obigen Features ergibt sich der folgende Datensatz, der von einem herkömmmlichen Klassifikationsalgorithmus verarbeitet werden kann:\\

\begin{tabular}{|c|c|c|c|c|}
	\hline 
	Intervall & $\text{Avg}_\text{Band, Gyroscope, x}$ & $\text{Avg}_\text{Band, Gyroscope, y}$  & $...$  & sampleClass  \\ 
	\hline 
	$I_1$ & $1.5$ & $1$ & $...$ & Jogging \\ 
	\hline 
	$I_2$ & $...$ & $...$ & $...$ & Jogging   \\ 
	\hline 
\end{tabular} \\\\
Die Intervallspalte dient nur der Illustration und wird dem ML-Algorithmus nicht gezeigt.

\section{Nachbearbeitung der Daten}
Bei der Transformation kann sich das Problem ergeben, dass nicht jede Zelle der resultierenden Tabelle einen Wert enthält. Dazu kann es beispielsweise kommen, wenn der Algorithmus \texttt{AverageTimeBetweenPeaks} \textit{Nil} zurückgibt. Enthält eine Zelle keinen Wert, so wird in ihr der Durchschnittswert des gefüllten Teils der Spalte eingesetzt. Um dem Risiko vorzubeugen, dass ML-Algorithmen Spalten mit betragsmäßig hohen Werten fälschlicherweise auch automatisch hohen Einfluss zuschreiben, werden alle numerischen Spalten mit Hilfe des z-Score-Verfahrens normalisiert. Dazu wird von den Werten jeder Spalte der Durchschnittswert dieser abgezogen, wonach das Resultat durch die Standardabweichung der Spalte geteilt wird. Der Betrag einer Zelle gibt danach an, wie viele Standardabweichungen vom Durchschnitt abgewichen wird.

\section{Anwendung von ML-Klassifikationsalgorithmen}
\label{section:ml-algos}

Abbildung~\ref{fig:recording-and-transformation} zeigt noch einmal abschließend den Aufnahme- und Transformationsprozess bis zu diesem Schritt. Erst in der Tabellenform, die im untersten Teil der Abbildung zu sehen ist, können die gesammelten Daten von einem herkömmlichen ML-Klassifikationsalgorithmus zum Training und außerdem zur Evaluation des damit gelernten Modells verwendet werden.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth,clip,trim=5mm 5mm 5mm 5mm]{img/transformation}
	\caption{Aufnahme- und Transformationsprozess}
	\label{fig:recording-and-transformation}
\end{figure}

Um die Vergleichbarkeit mit \cite{Weiss2016} sicherzustellen, wurde die Java-basierte ML-Software WEKA zum maschinellen Lernen verwendet. Diese bietet fertige Implementierungen diverser ML-Algorithmen an, die sowohl mittels einer grafischen Benutzeroberfläche, als auch in Code verwendet werden können. Die in dieser Arbeit verwendeten Algorithmen entsprechen denen aus \cite{Weiss2016} und werden im Folgenden kurz beschrieben. Falls nicht anders angegeben, wurden für jedes Modell die standardmäßigen Hyperparameter verwendet. Dabei handelt es sich um Parameter, die der jeweilige Algorithmus nicht selbst lernen kann.
\todo{Hyperparameter aktualisieren, falls nötig}
\subsection{Random Forest (RF)}
Ein \textit{Random Forest} ist ein Wald von Entscheidungsbäumen (erklärt in Kapitel 14.4 \cite{Bishop2006}), die randomisiert entstanden sind. Soll eine neue Instanz klassifiziert werden, treffen alle Entscheidungsbäume eine Entscheidung und die am häufigsten vorhergesagte Klasse gewinnt. WEKA nutzt eine Implementierung nach \cite{Breiman2001}. Experimentell haben sich die folgenden Hyperparameterwerte als sinnvoll erwiesen: Die Anzahl der Bäume in Wald wurde auf 50, die Anzahl der verwendeten Features pro Baum auf 10 sowie die maximale Tiefe auf 25 festgelegt.
\subsection{J48-Entscheidungsbaum (J48)}
J48 ist eine Implementierung des \textit{C4.5}-Algorithmus \cite{Quinlan1993}. Der Entscheidungsbaum wird anhand von Trainingsdaten $T$ iterativ durch das Hinzufügen von Entscheidungsknoten erzeugt, die den Informationsgehalt der Teilmengen von $T$ minimieren, die aus dem Split an jenem Entscheidungsknoten hervorgehen. Der Informationsgehalt ist umso höher, je mehr verschiedene Klassen sich in einer Menge von Instanzen befinden. Ein niedriger Informationsgehalt besagt demnach, dass nach einer Entscheidung an einem Entscheidungsknoten klarer geworden ist, welcher Klasse die Instanzen in den Mengen nach dem Split zuzuordnen sind.
\subsection{Instance-Based-Learner (IB$k$)}
Der IB$k$-Algorithmus ist ein $k$NN-basierter Klassifikationsalgorithmus. Soll einer neuen Instanz $x \in \mathbb{R}^n$ eine Klasse zugeordnet werden, werden anhand eines Distanzmaßes für den Vektorraum $\mathbb{R}^n$ die $k$ nächsten Nachbarn von $x$ gesucht. $x$ wird danach die Klasse zugeordnet, die unter den $k$ Nachbarn am häufigsten vorkam. Weitere Details sind zu finden in \cite{Aha1991}.
$k$ ist dabei ein Hyperparameter, der auf 3 eingestellt wurde.
\subsection{Naive Bayes (NB)}
Naive Bayes ist eine probabilistische Methode zur Klassifikation \cite{John1995}. Mit $x = (x_1, ..., x_n)$ wird die Klasse $C$ gesucht, die $\mathbb{P}(C | x_1, ..., x_n)$ maximiert. $x$ besteht hier aus nominalen Attributen. Der Satz von Bayes besagt, dass $\mathbb{P}(C|x) = \frac{\mathbb{P}(C) \mathbb{P}(x | C)}{\mathbb{P}(x)}$. Da über $C$ optimiert wird, kann der Nenner für diesen Ansatz ignoriert werden. Nimmt man \textit{naiv} an, dass $x_1, ..., x_n$ voneinander unabhängig sind, kann somit statt $\mathbb{P}(C | x)$ auch folgender Term über $C$ maximiert werden: $\mathbb{P}(C) \prod_{i=1}^{n} \mathbb{P}(x_i | C)$. Durch den Trainingsdatensatz sind die $\mathbb{P}(C)$ als Prior der Klassen $C$ sowie alle $\mathbb{P}(x_i | C)$ bekannt, sodass der Term durch einfaches Ausprobieren aller Klassen maximiert werden kann.

Enthält $x$ numerische Attribute, müssen diese zunächst durch nominale Attribute ersetzt werden. Der Wertebereich aller $x_i$ über den Trainingsdatensatz hinweg kann in eine feste Anzahl von Intervallen aufgeteilt werden. Für jedes Intervall $I$ wird nun eine Indikatorvariable als Feature angelegt, die angibt, ob für eine Instanz $x$ gilt, dass $x_i \in I$.
\subsection{Multilayer Perceptron (MLP)}
Bei dieser Methode handelt es sich um ein mehrschichtiges Netzwerk von sogenannten \textit{Perzeptren}, auch \textit{(künstliches) neuronales Netzwerk} genannt. Ein Perzeptron erhält $n$ Eingaben $x_1, ..., x_n$ und gewichtet diese mit Parametern $\Theta_1, ..., \Theta_n$, die gelernt werden. Das Perzeptron berechnet $y = g(\sum_{i=1}^{n} \Theta_i x_i)$ als Ausgabewert, wobei $g$ eine Aktivierungsfunktion ist, die den Ausgabewert beschränkt, beispielsweise die Sigmoidfunktion.

Die Ausgabe eines Perzeptrons kann einem Perzeptron der nächsten Schicht als Eingang dienen. Die letzte Schicht fungiert als Ausgabe des Netzwerks, dessen Fehler anhand eines Trainingsdatensatzes bestimmt werden kann. Jedes Perzeptron berechnet, wie die eigenen Parameter $\Theta$ angepasst werden müssen, um den Fehler zu minimieren, wofür der Fehler der Perzeptren der letzten Schicht an die vorherige Schicht weiter propagiert wird (\textit{Backpropagation}). Zur Optimierung der Parameter wird ein stochastischer Gradientenabstieg verwendet. Zur Klassifikation können die Ausgangswerte der Perzeptren der letzten Schicht als Indikatorvariablen für die vorhandenen Klassen interpretiert werden. Weitere Informationen sind zu finden in \cite{Haykin1994}.